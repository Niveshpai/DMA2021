{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of Traveler dataset\n",
    "\n",
    "* **Goal:** Predict the country that users will make their first booking in, based on some basic user profile data.\n",
    "\n",
    "\n",
    "* **Training data:** set of users with correct category (i.e. what country they made their first booking in).\n",
    "\n",
    "\n",
    "* **Build a model:** to accurately predict the country of first booking.\n",
    "\n",
    "\n",
    "* **Test data:** set of users without the knowledge of outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of Data Scientist process\n",
    "\n",
    "#### [1] Pre-processing: Assessing and analyzing (Understanding) data, cleaning, transforming and adding new features\n",
    "#### [2] Learning model: Constructing and testing learning model\n",
    "#### [3] Post-processing: Creating final predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First Task in hand:** Preprocessing, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone1: Understanding the Data\n",
    "\n",
    "#### Formulate range of questions including (but not limited to):\n",
    "\n",
    "   1. What features (columns) does the dataset contain?\n",
    "   \n",
    "   \n",
    "   2. How many records (rows) have been provided?\n",
    "   \n",
    "   \n",
    "   3. What format is the data in (e.g. what format are the dates provided, are there numerical values, what do the different categorical values look like)?\n",
    "   \n",
    "   \n",
    "   4. Are there missing values?\n",
    "   \n",
    "   \n",
    "   5. How do the different features relate to each other?\n",
    "    \n",
    "    Note: Look into the csv files provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import display, Math, Latex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the Dataset\n",
    "\n",
    "   1. **train_users_2.csv**  – This dataset contains data on Traveler users, including the *destination countries*. Each row represents one user with the columns containing various information such the users’ ages and when they signed up. This is the primary dataset used to train the model.\n",
    "   \n",
    "    \n",
    "   2. **test_users.csv** – This dataset also contains data on Traveler users, in the same format as train_users_2.csv, except without the destination country. These are the users for which final prediction model need to be tested.\n",
    "   \n",
    "   \n",
    "   3. **sessions.csv** – This data is supplementary data that can be used to train the model and make the final predictions. It contains information about the actions (e.g. clicked on a listing, updated a  wish list, ran a search etc.) taken by the users in both the testing and training datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Traveler data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the train_users_2.csv file\n",
    "data = pd.read_csv(\"./traveler_dataset/train_users_2.csv\")\n",
    "train_users = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the test_users.csv file\n",
    "test_users = pd.read_csv('./traveler_dataset/test_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many users are in training set and test set\n",
    "print(\"We have\", train_users.shape[0], \"users in the training set and\", \n",
    "      test_users.shape[0], \"in the test set.\")\n",
    "print(\"In total we have\", train_users.shape[0] + test_users.shape[0], \"users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_users.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at sample data what can be observed (but not limited)\n",
    "\n",
    "   1. Missing values in the **age** and **date_first_booking** attribute\n",
    "        - ?: need to be filled or the rows excluded altogether\n",
    "\n",
    "\n",
    "   2. Most of the attributes provided contain **categorical data**\n",
    "        - ?: 11 of the 16 attributes provided appear to be categorical\n",
    "        - Whats the problem? Most algs used in classification do not handle categorical data well. \n",
    "        - **Solution:** In data transformation, find ways to change data into forms more suitable for classification. \n",
    "\n",
    "\n",
    "   3. The **timestamp_first_active** attribute looks to be a full timestamp\n",
    "        - ?: For example 20090609231247 looks like it should be 2009-06-09 23:12:47\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the structure of the Traveler data\n",
    "\n",
    " ### 1. Country Destination Values\n",
    "\n",
    "- **country_destination** attribute (Most important) \n",
    "\n",
    "- Why? - Looking at the number of records that fall into each category can help provide some insights into how the model should be constructed as well as pitfalls to avoid.\n",
    "\n",
    "<img src=\"./images/User_by_Destination.png\" height=\"400\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: HW\n",
    "  - Create the above table using Matplotlib or Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: Looking at the breakdown of the data, one thing that immediately stands out is that almost 90% of users fall into two categories, that is, they are either yet to make a booking (NDF) or they made their first booking in the US. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What’s more, breaking down these percentage splits by year reveals that the percentage of users yet to make a booking (i.e., NDF) increases each year and reached over 60% in 2014.\n",
    "\n",
    "<img src=\"./images/User_by_Destination_and_Year.png\" height=\"400\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: HW\n",
    "  - Create the above table using Matplotlib or Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for building a learning model:\n",
    "\n",
    "   ##### [1] By analysis we observe that the spread of categories (yearwise) might have changed over time.  Since the final predictions will be made against user data from 2014 onwards, we can focus on more recent data for training the learning model as it is more likely to resemble the test data.\n",
    "   \n",
    "   ##### [2] Since vast majority of users fall into 2 categories ('NDF' and 'US') there is a risk that if the learning model is too generalized, it will select one of these two categories for every prediction.\n",
    "    \n",
    "   \n",
    "**Important:** Ensure that the training data has enough information to build a learning model that will predict other categories as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Account Creation Dates\n",
    " - **date_account_created** attribute – how values have changed over time?\n",
    " \n",
    "<img src=\"./images/Accounts_Created_Over_Time.png\" height=\"400\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: HW\n",
    "  - Create the above line graph using Matplotlib or Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "   ##### [1] We observe that there is an explosive growth, averaging over 10% growth in new accounts create per month. \n",
    "   ##### [2] In 2014 there is rapid increase from the years before.\n",
    "   \n",
    "   ##### In fact, we can limit considering the training data to accounts created from Jan 2013 onwards (70% will still be included)\n",
    "   **Note:** Looking back to **sessions.csv** we would be limited to data from Jan 2014 onwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Age Breakdown\n",
    "\n",
    "#### Data Quality issue: \n",
    "- some users reported their ages well over 100 and 1000 \n",
    "\n",
    "\n",
    "<img src=\"./images/Reported_Ages_of_Users.png\" height=\"400\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: HW\n",
    "  - Create the above bar graph using Matplotlib or Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "   ##### [1] Appears that a number of users have reported their birth year instead of their age.\n",
    "   \n",
    "   \n",
    "   ##### [2] Significant numbers of users reporting their age > 100 .\n",
    "   - Why? - might be some users intentionally entered their age incorrectly for privacy reasons.\n",
    "   - These are errors and needs to be addressed.\n",
    "   \n",
    "   \n",
    "   ##### [3] Another issue with the age attribute is that sometimes age has not been reported at all.\n",
    "   - Check missing ages? \n",
    "   - Large number of missing values in all years.\n",
    "   \n",
    "   <img src=\"./images/Missing_Ages.png\" height=\"400\" width=\"500\"/>\n",
    "   \n",
    "   **Note:** While cleaning the data, need to decide what to do with these missing values.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. First Device Type\n",
    " - **first_device_used** attribute\n",
    " \n",
    "<img src=\"./images/First_Device_Used.png\" height=\"400\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: HW\n",
    "  - Create the above table using Matplotlib or Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "##### [1] MAC and Windows Desktop users have increased significantly as a percentage of all users.\n",
    "##### [2] iPhone users have tripled from 2010 to 2014.\n",
    "##### [3] Users using ‘Other/unknown’ devices have gone from the second largest group to less than 5% of users from 2010 to 2014.\n",
    "\n",
    "#### Again, we can notice that the recent data is likely to be most useful for building the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: HW\n",
    "- **Other attributes:** Give a look on other attributes and see how they can also help in building an accurate classification learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 2:  Focus on Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Fixing up timestamp formats \n",
    "\n",
    "  - **timestamp_first_active** attribute contains number like 20090609231247 instead of timestamps in the expected format: 2009-06-09 23:12:47\n",
    "  \n",
    "### [2] Filling in missing values \n",
    "\n",
    "### [3] Correcting erroneous values  \n",
    "\n",
    "  - **gender** attribute where some have entered *'-unknown-'*. \n",
    "  - **age** attribute where some have entered value well over *100*. \n",
    "\n",
    "### [4] Standardizing categories \n",
    "  \n",
    "  - spelling mistakes, language differences or other factors will result in a given answer being provided in multiple ways.\n",
    "  - **Example:** data on country of birth, if users are not provided with a standardized list of countries, the data will inevitably contain multiple spellings of the same country (e.g. USA, United States, U.S. and so on)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data - Solution for [2]\n",
    "\n",
    "#### [1] Deleting/Ignoring rows with missing values\n",
    "  - a. If more than 10% of data to be deleted, then try reconsidering.\n",
    "  - b. Be confident that the rows being deleted do not contain information which is not contained in other rows.\n",
    "       - For example, in the dataset we observe that many users have not provided their age. \n",
    "       - Can we assume that the people who chose not to provide their age are the same as the users who did? \n",
    "       -  Or are they likely to represent a different type of user, perhaps an older and more privacy conscious user, and therefore a user who is likely to make different choices on which countries to visit? \n",
    "       - If the answer is the latter, we probably do not want to just delete the records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] Filling in the missing values\n",
    "- What value to use?\n",
    "\n",
    "- Depends on a range of factors, including the type of data we are trying to fill.\n",
    "\n",
    "- **Categorical:** \n",
    "    - If the data is categorical (i.e. countries, device types, etc.), it may make sense to simply create a new category that will represent ‘unknown’.\n",
    "\n",
    "    - Another option may be to fill the values with the most common value for that attribute (use mode).\n",
    "\n",
    "    - Since these are broad methods for filling the missing values, they may **oversimplify** your data and/or make your final learning model less accurate.\n",
    "\n",
    "- **Numerical:** \n",
    "    - For example age attribute, we could use mean or median to fill values.\n",
    "    \n",
    "    - Or, take an average based on some other criteria – for example filling the missing age values based on an average age for users who selected the same country_destination.\n",
    "\n",
    "**Note:** For both types of data, we can use far more complicated methods to impute the missing values. There are endless ways...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: HW\n",
    "- **Impute the missing values:** Explore and list the other ways of filling missing values for boh Numerical and Categorical data largely practiced in competetions for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List solutions:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning efforts on two files\n",
    "\n",
    " - **train_users_2.csv** and \n",
    " - **test_users.csv** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "\n",
    "print(\"Reading data...\")\n",
    "train_file = \"./traveler_dataset/train_users_2.csv\"\n",
    "df_train = pd.read_csv(train_file, header = 0,index_col = None)\n",
    "\n",
    "test_file = \"./traveler_dataset/test_users.csv\"\n",
    "df_test = pd.read_csv(test_file, header = 0,index_col = None)\n",
    "\n",
    "# Combining into one dataset for cleaning\n",
    "df_all = pd.concat((df_train, df_test), axis = 0, ignore_index = True, sort = False)\n",
    "print(\"Reading data...completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8:\n",
    "- Do few operations like displaying the attributes, sample rows, finding NaNs, attribute statistics, no. of rows, columns, etc in dataframes \n",
    "  - **df_train**\n",
    "  - **df_test**\n",
    "  - **df_all**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cleaning the timestamps - Fixing up formats of dates\n",
    "\n",
    "- Why to convert? \n",
    "   - Reason: e.g. subtracting one date from another, extracting the month of the year from each date, etc.\n",
    "\n",
    "**Note:** In next exercise, we will find its importance when we start adding various new features to the training data based on this date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing date formats in Pandas using to_datetime\n",
    "## Change dates to specific format\n",
    "\n",
    "print(\"Fixing timestamps...\")\n",
    "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'], format = '%Y-%m-%d')\n",
    "df_all['timestamp_first_active'] = pd.to_datetime(df_all['timestamp_first_active'], format = '%Y%m%d%H%M%S')\n",
    "print(\"Fixing timestamps...completed\")\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do following step only if date_account_created is empty\n",
    "## date_account_created attribute is sometimes empty\n",
    "\n",
    "## Solution: replace the empty values with the value in the timestamp_first_active attribute using the fillna() function\n",
    "# df_all['date_account_created'].fillna(df_all.timestamp_first_active, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Removing booking date field\n",
    "\n",
    "- Why? Notice howmany date fields are there?\n",
    "\n",
    "- We converted two date fields to a format above.\n",
    "\n",
    "- Which one is not covered?\n",
    "\n",
    "- Why? \n",
    "  - **Reason1:** In **training_users_2.csv**, all the users who have a first booking **country_destination** have a value in the **date_first_booking** attribute AND those who have not made a booking (i.e., **country_destination = NDF**) the value is missing. \n",
    "  \n",
    "  - **Reason2:** In **test_users.csv**, the **date_first_booking** attribute is empty for all the records.\n",
    "\n",
    "\n",
    "#### Analysis:\n",
    "\n",
    "  - **date_first_booking** attribute is not going to be useful for predicting which country a booking will be made. \n",
    "  \n",
    "  - What is more, if we include **date_first_booking** attribute in the training dataset when building the model, it will likely increase the chances that the model predicts *NDF* as those are the records without dates in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing date_first_booking attribute\n",
    "df_all.drop('date_first_booking', axis = 1, inplace = True)\n",
    "print(\"Droped date_first_booking attribute...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting erroneous values - Solution for [3] (Cleaning the Age attribute)\n",
    "\n",
    "- [1] **Outliers** - there are several age values that are clearly incorrect (unreasonably high or too low)\n",
    "     - **Solution:** replace these incorrect values with 'NaN' (changing incorrect values into missing values)\n",
    "\n",
    "\n",
    "- [2] **Missing values** - there are a significant number of users who did not provide their age at all, they show up as NaN in the dataset\n",
    "     - **Solution:** change all the NaN values to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove outliers function - [1]\n",
    "def remove_outliers(df, column, min_val, max_val):\n",
    "    col_values = df[column].values\n",
    "    df[column] = np.where(np.logical_or(col_values <= min_val, \n",
    "                                        col_values >= max_val), \n",
    "                          np.NaN, col_values)\n",
    "    return df\n",
    "\n",
    "print(\"Removing outliers in age attribute with NaN...\")\n",
    "df_all = remove_outliers(df = df_all, column = 'age', min_val = 15, max_val = 90)\n",
    "print(\"Removing outliers in age attribute with NaN...completed\")\n",
    "\n",
    "## Fixing age column - [2]\n",
    "print(\"Fixing age attribute...\")\n",
    "df_all['age'].fillna(-1, inplace = True)\n",
    "print(\"Fixing age attribute...completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.age.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 9: HW\n",
    "- There are several more ways to fill in the missing values in the age attribute, try and list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in missing values: Identify and fill additional attributes with missing values \n",
    "\n",
    "- One such attribute is **first_affiliate_tracked** has missing values\n",
    "  - **Solution:** follow same procedure as above (change all the NaN values to -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(20).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10:\n",
    "- Are there any missing values in **first_affiliate_tracked** attribute, if yes how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11:\n",
    "- Fill **first_affiliate_tracked** attribute by following same procedure as above (change all the NaN values to -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill first_affiliate_tracked attribute\n",
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the data look like after all these changes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample of some rows from cleaned dataset\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Is that all?\n",
    "- **Not really** - this is just a small work of cleaning, you need to try more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Next?\n",
    "- **Aim:** Focus on transforming the data and feature extraction\n",
    "   - **Why?** To build better prediction learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Transformation and Feature Extraction as a Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Why transformation?\n",
    "\n",
    "- Unlike the steps taken during cleaning, which are designed to address problems with the raw data (missing and erroneous values, formatting issues etc.), \n",
    "\n",
    "- Data Transformation steps change the values and/or structure of the data (data transformation) and add additional features (feature extraction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Transformation methods:\n",
    "\n",
    "#### [1] Bucketing/Binning\n",
    "\n",
    "- the numerical values in a particular attribute are converted from a continuous series into fixed ranges\n",
    "- example, instead of using the age value of all users, we could place them into buckets such as 15-20 years old, 21-25 years old and so on\n",
    "\n",
    "#### [2] Normalization\n",
    "\n",
    "#### [3] Other Transformations\n",
    "\n",
    "- There are unlimited number of ways that the numerical values of a given attribute can be transformed such that they are more suitable for the algorithm being used.\n",
    "\n",
    "- Logarithm function: This transformation is a commonly used method of dealing with exponential data series (i.e. a column where there are lot of low values and relatively few high values). \n",
    "\n",
    "#### [3.1] One Hot Encoding (used for categorical data)\n",
    "<img src=\"./images/One_Hot_Encoding.png\" height=\"400\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature Extraction:\n",
    "- feature construction and feature selection to add to dataset\n",
    "\n",
    "#### [1] Using Hierarchical Information\n",
    "\n",
    " - data in dataset represents one level of hierarchy, and extracting other implied levels of that hierarchy will provide the learning model with useful information\n",
    " \n",
    " - Eg: Consider date fields,  \n",
    "     - extracting the day of the week, \n",
    "     - the month of the year,\n",
    "     - the hour of the day, \n",
    "     \n",
    " - could add important information for the algorithm to use\n",
    "     - Maybe people who create their accounts in summer months are more likely to make a booking in a warmer country. \n",
    "     - Maybe people who were first active late at night are more disorganized travelers and are therefore more likely to make a domestic first booking. \n",
    "     - Combination of these factors may make the difference (e.g. users first active late at night, in the summer months, on a weekday are more likely to travel to Portugal). \n",
    "   \n",
    "   #### Note:The point is not to be able to explain why a factor may be important, but to think of as many factors as possible to test, and allow the algorithm to determine what is important.\n",
    "    \n",
    "#### [2] Adding External Data\n",
    "\n",
    " - (known as record linkage) data enrichment\n",
    " \n",
    " - Eg: Countries could be enriched with demographic data about the country such as \n",
    "      - population, \n",
    "      - income per capita or \n",
    "      - land area \n",
    " - all the factors that may allow the algorithm to draw conclusions across similar groups of countries.\n",
    " \n",
    " - Consider how much more accurately we could predict a first booking country of a user if we could link the data from their TRAVELER profile to data from one of their social media profiles (Facebook, Twitter etc.) or even better, from other Traveler accounts.\n",
    " \n",
    " \n",
    " #### IMPORTANT:\n",
    " \n",
    " - The key point here is that it is worth investing time looking for ways to add new and useful data to your existing dataset before moving onto the learning modeling step. \n",
    " \n",
    " - Expanding your dataset in this manner will often produce far bigger improvements in prediction accuracy than the choice of algorithm or the tuning of the algorithm parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Practical: Transforming Categorical Data\n",
    "#### One Hot Enconder method \n",
    "\n",
    "   - replacing the categorical fields in the dataset with multiple columns representing one value from each column\n",
    "\n",
    "   - with Scikit Learn (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "   \n",
    "   - own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prototype for only gender attribute  - testing\n",
    "# User defined One Hot Encoding function\n",
    "def convert_to_binary(df,column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    print (categories)\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        print (col_name)\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "#columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "columns_to_convert = ['gender']\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    #df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"One Hot Encoding categorical data...completed\")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined One Hot Encoding function\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"One Hot Encoding categorical data...completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating New Features\n",
    "\n",
    "- Using two date fields – **date_account_created** and **timestamp_first_active**\n",
    "\n",
    "- Extract all the information we can out of these two date fields that could potentially differentiate which country someone will make their first booking in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new date related fields\n",
    "print(\"Adding new fields...\")\n",
    "df_all['day_account_created'] = df_all['date_account_created'].dt.weekday\n",
    "df_all['month_account_created'] = df_all['date_account_created'].dt.month\n",
    "df_all['quarter_account_created'] = df_all['date_account_created'].dt.quarter\n",
    "df_all['year_account_created'] = df_all['date_account_created'].dt.year\n",
    "df_all['hour_first_active'] = df_all['timestamp_first_active'].dt.hour\n",
    "df_all['day_first_active'] = df_all['timestamp_first_active'].dt.weekday\n",
    "df_all['month_first_active'] = df_all['timestamp_first_active'].dt.month\n",
    "df_all['quarter_first_active'] = df_all['timestamp_first_active'].dt.quarter\n",
    "df_all['year_first_active'] = df_all['timestamp_first_active'].dt.year\n",
    "df_all['created_less_active'] = (df_all['date_account_created'] - df_all['timestamp_first_active']).dt.days\n",
    "print(\"Adding new fields...completed\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "print(\"Droping fields...\")\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_all.columns:\n",
    "        df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"Droping fields...completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "- Changed training dataset from 15 columns to 164 columns\n",
    "- **HW:** Investigate what information could be extracted from the other non-date columns?\n",
    "- **HW:** Check what external data could be added? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **HW:** With dummy variables created by One-hot-encoding are we introducing multicollinearity ?\n",
    "    - If yes... can we use PCA to reduce the dimensionality down to prevent the model from being misled?\n",
    "    \n",
    "- **HW:** How worried about multicollinearity you need to be if you are (i) building a regression model and (ii) concerned with prediction accuracy ?\n",
    "\n",
    "- **Example:** StackExchange multicollinearity: https://stats.stackexchange.com/questions/168622/why-is-multicollinearity-not-checked-in-modern-statistics-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Next? Data Integration.\n",
    "### Aim: Understanding the sessions.csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
